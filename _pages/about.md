---
permalink: /
author_profile: true
image: ../images/ai-sec.png
title: AI Privacy and Security Seminars
---

<br>This is the homepage of the **AI Privacy & Security (AI-PriSec) Interest Group**. It brings together researchers and practitioners around the world broadly interested in this topic. For the time being, it features recurring seminars, a couple of times a month, always on Sunday, around 10 AM (HK Time).  

### Get Involved
- Subscribe to our **[WeChat Public Account]()** to receive to seminar and job announcements
- Join our **[WeChat Group](https://drive.google.com/file/d/1SC-7k0xuDVc7r8B9nFeDPQVoxMQdPOGe/view?usp=sharing)** &ndash; this is particularly useful for students, who maintain an active working group with monthly (virtual) meet-ups
- Subscribe to our **[Bilibili Channel](https://space.bilibili.com/341626036?spm_id_from=333.337.0.0)** where we live stream and keep recordings of the talks




### Upcoming Seminars

<img src="../images/yiliu.jfif" style="float:right;width:100px;height:100px;margin-top:00px">
- 6 March 2022, 10:00 ([HK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220206T10&p1=102))  
**Yi Liu (CityU)**  
BadBatch: Practical Data Poisoning Attack against Federated Learning via Manipulating Local Training Batch    
[[WeMeet Registration](https://meeting.tencent.com/dw/mD5ht1WOQQbQ)] [[Live Stream](https://meeting.tencent.com/dw/mD5ht1WOQQbQ)]<details><br>**Abstract:** Federated learning (FL) as a privacy-friendly collaborative learning framework benefits machine learning powered systems and services. Despite its advantages, FL is known to be vulnerable to poisoning attacks, where the adversary controls a set of clients to poison either the local training dataset or the local model update to degrade the global model performance. Existing poisoning attacks have demonstrated vast damage to FL, but they either require strong adversarial assumption on model and dataset knowledge of a certain number of clients, or rely on optimization-based attack methods that are normally expensive in a decentralized environment. In this paper, we propose a new practical poisoning attack against FL, named BadBatch, which can be launched in realistic settings of FL and does not rely on optimization. Our key observation is that by simply manipulating the local training batch, an adversary is capable of influencing global model performance and convergence. We propose gradient-oriented and model update-oriented attack strategies that focus on increasing the stochastic error of stochastic gradient descent and forcing the model to forget learned generalization features by finding bad batches. We also theoretically analyze the error upper bound and time complexity of our attack. Finally, we perform extensive experiments on two public datasets for convex and non-convex models, and evaluate our attacks against the latest defense, i.e., Byzantine robust aggregation. Our evaluation results show that Badbatch can achieve high attack performance than existing methods in a practical FL setting, shedding light on data poisoning attacks against practical FL.<br><br>**Bio:** TBA<br></details>


<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/Dn_NkH-IEVA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->



### Past Seminars

<img src="../images/Zijing.jpg" style="float:right;width:100px;height:100px;margin-top:00px">
- 27 February 2022, 10:00 ([HK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220228T10&p1=102))  
**Zijing Ou (Tencent AI Lab)**  
Model Explanation with Shapley Values  
[[Recording](https://space.bilibili.com/341626036?spm_id_from=333.337.0.0)] [[Slide](https://drive.google.com/file/d/1L6idiKFWLioqKesi7zZ2qu5ewB9doEY6/view?usp=sharing)]<details><br>**Abstract:** Deep neural networks (DNNs) become increasingly important in many applications while lacking explanations for their excellent performance. **Shapley Value** provides a theoretical and practical explainer for DNNs. In this talk, the presenter will introduce the most recent progress in model explanation with Shapley value, including its estimation, uncertainty, and potential research areas.<br><br>**Bio:** Zijing Ou recently graduated with a B.E. degree from Sun Yat-sen University and now works as an intern in Tencent AI Lab. His research interests include approximate inference, energy-based models, and interpretable AI. His research has been published at venues including IJCAI, ACL, EMNLP, etc. He also works as a reviewer for ICML, IJCAI, ACL, etc. Home: [https://j-zin.github.io/](https://j-zin.github.io/)<br>


### Organizers
- Prof. Yi Wu, HLJU, Harbin
- [Prof. Jiawen Kang](https://teacher.gdut.edu.cn/kangjiawen/zh_CN/index/204229/list/index.htm), GUDT, Guangzhou
- [Mr. Yi Liu](https://yiliucs.github.io/), CityU, Hong Kong
- [Mr. Zijing Ou](https://j-zin.github.io/), Tencent AI Lab, Shenzhen
- TBA



---
permalink: /
author_profile: true
image: ../images/photo.jpeg
title: Privacy and Security in ML Seminars
---

<br>This is the homepage of the **Privacy & Security in Machine Learning (PriSec-ML) Interest Group**. It brings together researchers and practitioners around the world broadly interested in this topic. For the time being, it features recurring seminars, a couple of times a month, always on Wednesdays, around 1.30 PM (London Time).  

### Get Involved
- Subscribe to our **[mailing list](https://www.jiscmail.ac.uk/cgi-bin/webadmin?SUBED1=PRISEC-ML&A=1)** to receive to seminar and job announcements
- Join our **[Slack](https://prisec-ml.slack.com)** by using this [link](https://join.slack.com/t/prisec-ml/shared_invite/zt-11jq2lt81-pv~MoVSbhaIIEhnX91uVSw) &ndash; this is particularly useful for students, who maintain an active working group with monthly (virtual) meet-ups
- Follow us on **[Twitter](https://twitter.com/prisec_ml)**
- Subscribe to our **[YouTube channel](http://youtube.com/c/PrivacyandMachineLearningInterestGroup)** where we live stream and keep recordings of the talks


### Upcoming Seminars

<img src="../images/nando.png" style="float:right;width:100px;height:100px;margin-top:00px">
- 9 February 2022, 13:30 ([UK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220209T1330&p1=136))  
**Ferdinando Fioretto (Syracuse University)**  
Privacy-preserving ML and decisions-making: uses and unintended disparate effects  
[[Zoom Registration](https://ucl.zoom.us/meeting/register/tJctf-murDwjEtRdhivmVyZMNFOeGKNP6xHs)] [[Live Stream](https://youtu.be/RRvG9As8q6s)]<details><br>**Abstract:** Since its conception, differential privacy (DP) has become an important privacy-enhancing technology for private analysis tasks. Several private companies and federal agencies are rapidly developing their implementation of DP, including the notable adoption by the US Census Bureau for their 2020 release. Importantly, the DP data or models released do not live in a vacuum. They are often used to make important policy decisions with significant societal and economic impacts for the involved individuals. For example, US census data users rely on the decennial census data to apportion congressional seats, allocate a multi-trillion budget, and distribute critical resources to states and jurisdictions. When this data is protected using differential privacy, the resulting decisions may have significant errors with disproportionate societal and economic impacts for participating individuals. In this talk, I will focus on the challenge of releasing privacy-preserving data sets for complex data analysis or learning tasks. I will discuss the impact that differential privacy may have on the fairness of a downstream decision process, analyze the sources of this issue, examine the conditions under which decision-making is fair when using differential privacy, and propose several mitigation approaches to either alleviate or bound unfairness. Finally, I will conclude with some open questions.<br><br>
**Bio:** Ferdinando Fioretto is an Assistant Professor of Computer Science at Syracuse University. He works on optimization, differential privacy, and machine learning. Ferdinando's recent work focuses on how to make AI algorithms better aligned with societal values, especially privacy and fairness, and how to integrate machine learning with optimization to  solve complex optimization problems. He is the recipient of the 2021 Mario Gerla Young Investigator Award by ISSNAF, the 2021 Early Career Researcher Award by the Association for Constraint Programming,  a Best AI dissertation award (AI*IA 2017), and several best paper awards. Among other activities, he co-organizes the annual AAAI privacy-preserving AI (PPAI) workshop. Home Page: [https://web.ecs.syr.edu/~ffiorett/](https://web.ecs.syr.edu/~ffiorett/)<br></details>

<img src="../images/giulia.png" style="float:right;width:100px;height:100px;margin-top:00px">
- 23 February 2022, 13:30 ([UK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220223T1330&p1=136))  
**Giulia Fanti (Carnegie Mellon University)**  
Locally Differentially-Private Sparse Vector Aggregation  
[[Zoom Registration](https://ucl.zoom.us/meeting/register/tJErcOCqrT0sHtQVGMYUhL40SFRimrujwpfl)] [[Live Stream](https://youtu.be/dnFa92daKxA)]<details><br>**Abstract:** Vector mean estimation is a central primitive in federated analytics. In vector mean estimation, each user holds a real-valued vector and a server wants to estimate the mean of all vectors, without compromising any individual user’s privacy. In this paper, we consider the k-sparse version of the vector mean estimation problem. That is, suppose each user’s vector has at most k non-zero coordinates in its d-dimensional vector, with k << d. In practice, since the universe size d can be very large (e.g., the space of all possible URLs), we would like the per-user communication to be succinct, i.e., independent of or (poly-)logarithmic in the universe size d. In this paper, we show matching upper and lower bounds for the k-sparse vector mean estimation problem under local differential privacy. Specifically, we construct new mechanisms that achieve asymptotically optimal error and succinct communication, either under user-level-LDP or event-level-LDP. Our experiments on real and synthetic data show 1-2 orders of magnitude reduction in error compared to prior works under typical parameter choices, while incurring mild communication costs.<br><br>**Bio:** Giulia Fanti is an Assistant Professor of Electrical and Computer Engineering at Carnegie Mellon University. Her research interests span the security, privacy, and efficiency of distributed systems. She is a two-time fellow of the World Economic Forum’s Global Future Council on Cybersecurity and a member of NIST's Information Security and Privacy Advisory Board. Her work has been recognized with best paper awards (Sigmetrics and MobiHoc) and faculty research awards from the Sloan Foundation, Intel, the U.S. Air Force Research Laboratory, Google, and JP Morgan Chase. She obtained her Ph.D. in EECS from U.C. Berkeley and her B.S. in ECE from Olin College of Engineering. Home: [https://www.ece.cmu.edu/directory/bios/fanti-giulia.html](https://www.ece.cmu.edu/directory/bios/fanti-giulia.html)<br></details>


<img src="../images/gang.jpg" style="float:right;width:100px;height:100px;margin-top:00px">
- 9 March 2022, 13:30 ([UK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220309T1330&p1=136))  
**Gang Wang (University of Illinois at Urbana-Champaign)**  
Talk Details TBA  
[[Zoom Registration]( https://ucl.zoom.us/meeting/register/tJ0tfu2vqTkuHtLz3umqmD9NOTB8XKEqEjue)] [[Live Stream](https://youtu.be/00WeMyk24tg)]<details><br>**Abstract:**<br>**Bio:** [https://gangw.cs.illinois.edu](https://gangw.cs.illinois.edu)<br></details>


<img src="../images/heng.jpg" style="float:right;width:100px;height:100px;margin-top:00px">
- 23 March 2022, 15:30 ([UK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220323T1530&p1=136))  
**Heng Yin (UC Riverside)**  
Talk Details TBA  
[[Zoom Registration](https://ucl.zoom.us/meeting/register/tJIrduytpzwtE9Bycfnc9n4DLRFQbIbf3Isc)] [[Live Stream](https://youtu.be/4MtUbyN-vGo)]<details><br>**Abstract:**<br>**Bio:** [https://www.cs.ucr.edu/~heng/](https://www.cs.ucr.edu/~heng/)<br></details>

<img src="../images/suman.png" style="float:right;width:100px;height:100px;margin-top:00px">
- 4 May  2022, 14:00 ([UK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220504T1400&p1=136))  
**Suman Jana (Columbia University)**  
Talk Details TBA  
[[Zoom Registration](https://ucl.zoom.us/meeting/register/tJUqcuuhpzgtGN2VpZlAmM_8q6xkBh-VygM8)] [[Live Stream](https://youtu.be/j9ol3b3azD0)]<details><br>**Abstract:**<br>**Bio:** [https://www.cs.columbia.edu/~suman/](https://www.cs.columbia.edu/~suman/)<br></details>

**Google Calendar:** \[[html](https://calendar.google.com/calendar/embed?src=oormvn3d4hah013g6gd39pjpfk%40group.calendar.google.com&ctz=Europe%2FLondon)\] \[[ics](https://calendar.google.com/calendar/ical/oormvn3d4hah013g6gd39pjpfk%40group.calendar.google.com/public/basic.ics)\]



### Past Seminars

<img src="../images/xiong.jpg" style="float:right;width:100px;height:100px;margin-top:00px">
- 26 January 2022, 13:30 ([UK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20220126T1330&p1=136))  
**Li Xiong (Emory University)**  
Trustworthy Machine Learning with Both Differential Privacy and Certified Robustness  
[[Recording](https://youtu.be/JCPK4wh-C88)]<details><br>**Abstract:** While deep learning models have achieved great success, they are also vulnerable to potential manipulations, ranging from model inversion attacks that attempt to infer sensitive training data from a model, to adversarial example attacks that create manipulated data instances to deceive a model.  In this talk, I will present our recent work on achieving 1) differential privacy (DP) to ensure privacy of the training data and 2) certified robustness against adversarial examples for deep learning models.  First, I will present a practical DP training framework for centralized setting with better empirical and theoretical utility (IJCAI’21).  Second, I will present a certified robustness approach via randomized smoothing for quantized neural networks (ICCV ’21). Finally, I will present a framework that kills two birds with one stone and achieves DP and certified robustness via randomized smoothing simultaneously.
<br><br>**Bio:** Li Xiong is a Professor of Computer Science and Biomedical Informatics at Emory University. She held a Winship Distinguished Research Professorship from 2015-2018. She has a Ph.D. from Georgia Institute of Technology, an MS from Johns Hopkins University, and a BS from the University of Science and Technology of China. She and her research lab, Assured Information Management and Sharing (AIMS), conduct research on the intersection of data management, machine learning, and data privacy and security. She has published over 160 papers and received six best paper (runner up) awards. She has served and serves as associate editor for IEEE TKDE, VLDBJ, IEEE TDSC, general or program co-chairs for ACM CIKM 2022, IEEE BigData 2020, and ACM SIGSPATIAL 2018, 2020. Her research has been supported by National Science Foundation (NSF), AFOSR (Air Force Office of Scientific Research), National Institute of Health (NIH), and Patient-Centered Outcomes Research Institute (PCORI). She is also a recipient of Google Research Award, IBM Smarter Healthcare Faculty Innovation Award, Cisco Research Awards, AT&T Research Gift, and Woodrow Wilson Career Enhancement Fellowship.  She is an ACM distinguished member.  More details at [http://www.cs.emory.edu/~lxiong](http://www.cs.emory.edu/~lxiong).<br></details>


<img src="../images/eugene.jpg" style="float:right;width:100px;height:100px;margin-top:00px">
- 1 December 2021, 13:30 ([UK time](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Seminar&iso=20211201T1330&p1=136))  
**Eugene Bagdasaryan (Cornell Tech and Apple)**  
Blind Backdoors in Deep Learning  
[[Recording](https://youtu.be/TCBPX3CA5UQ)]<details><br>**Abstract:** We investigate a new method for injecting backdoors into machine learning models, based on compromising the loss-value computation in the model-training code. We use it to demonstrate new classes of backdoors strictly more powerful than those in the prior literature: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and backdoors that do not require inference-time input modifications.<br><br>
Our attack is blind: the attacker cannot modify the training data, nor observe the execution of his code, nor access the resulting model. The attack code creates poisoned training inputs "on the fly," as the model is training, and uses multi-objective optimization to achieve high accuracy on both the main and backdoor tasks. We show how a blind attack can evade any known defense and propose new ones.
<br><br>**Bio:** Eugene is a PhD Candidate at Cornell Tech advised by Vitaly Shmatikov and Deborah Estrin. He is an Apple AI/ML Scholar. He focuses on privacy and security implications of applying machine learning in the real world, specifically backdoor attacks and defenses, differential privacy, and federated learning.<br>[https://www.cs.cornell.edu/~eugene/](https://www.cs.cornell.edu/~eugene/)<br></details>

<img src="../images/reza.jpeg" style="float:right;width:100px;height:100px;margin-top:00px">
- 17 November 2021  
**Prof. Reza Shokri (National University of Singapore)**  
Advanced Membership Inference Attacks  
[[Recording](https://youtu.be/Ud7-sSJTG2k)]<details><br>**Abstract:** Data Protection regulations, such as GDPR, and AI governance frameworks require personal data to be protected when used in AI systems, and that the users have control over their data and awareness about how it is being used. For example, Article 35 of GDPR requires organizations to systematically analyze, identify and minimize the data protection risks of a project, especially when it involves innovative technologies such as deep learning. Thus, proper mechanisms need to be in place to quantitatively evaluate and verify the privacy of individuals in every step of the data processing pipeline in AI systems. In this talk, I will define what data privacy is in the context of machine learning, and how it can be quantified. I will also present ML Privacy Meter tool that enables quantifying the privacy risks of machine learning models. The tool produces privacy risk analysis reports, which help in identifying data records among the training data that are under high risk of being leaked through the model parameters or predictions.<br><br>
The privacy risk analysis in ML Privacy Meter is based on membership inference attacks. In this talk, I present a formal framework to reason about such attacks, and introduce new powerful attacks that outperform the existing attacks and are designed to precisely quantify the information leakage through models. This is a joint work with Jiayuan Ye, Aadyaa Maddi, and Sasi Murakonda<br><br>**Bio:** Reza Shokri is a NUS Presidential Young Professor of Computer Science. His research focuses on data privacy and trustworthy machine learning. He is a recipient of the IEEE Security and Privacy (S&P) Test-of-Time Award 2021, for his paper on quantifying location privacy. He received the Caspar Bowden Award for Outstanding Research in Privacy Enhancing Technologies in 2018, for his work on analyzing the privacy risks of machine learning models. He received the NUS Early Career Research Award 2019, VMWare Early Career Faculty Award 2021, Intel Faculty Research Award (Private AI Collaborative Research Institute) 2021-2022, and Google PDPO Faculty Award 2021. He obtained his PhD from EPFL.<br>
[https://www.comp.nus.edu.sg/~reza/](https://www.comp.nus.edu.sg/~reza/)<br></details>

<img src="../images/sara.png" style="float:right;width:100px;height:100px;margin-top:00px">
- 10 November 2021  
**Dr. Sara Hooker (Google Brain)**  
The myth of the interpretable, robust, compact and high performance DNN  
[[Recording](https://youtu.be/BZ3FDiXkP78)]<details><br>**Abstract:**<br>To-date, a discussion around the relative merits of different compression methods has centered on the trade-off between level of compression and top-line metrics. Along this dimension, compression techniques such as pruning and quantization are remarkably successful. It is possible to prune or heavily quantize with negligible decreases to test-set accuracy. However, top-line metrics obscure critical differences in generalization between compressed and non-compressed networks. In this talk, we will go beyond test-set accuracy and discuss some of my recent work measuring the trade-offs between compression, robustness and algorithmic bias. Characterizing these trade-offs provide insight into how capacity is used in deep neural networks -- the majority of parameters are used to represent a small fraction of the training set.<br><br>**Bio:**<br>[https://www.sarahooker.me](https://www.sarahooker.me)<br></details>

<img src="../images/konrad.jpeg" style="float:right;width:100px;height:100px;margin-top:0px">
- 13 October 2021  
**Prof. Konrad Rieck (Technische Universität Braunschweig)**  
Adversarial Preprocessing: Image-Scaling Attacks in Machine Learning  
[[Recording](https://youtu.be/kCKayHjZd3E)]<details><br>**Abstract:**<br>The remarkable advances of machine learning are overshadowed by attacks that thwart its proper operation. While previous work has mainly focused on attacking learning algorithms directly, another weak spot in intelligent systems has been overlooked: preprocessing. As an example of this threat, I present a recent class of attacks against image scaling. These attacks are agnostic to learning algorithms and affect the preprocessing of all vision systems that use vulnerable implementations, including versions of TensorFlow, OpenCV, and Pillow. Based on a root-cause analysis of the vulnerabilities, I introduce novel defenses that effectively block image-scaling attacks in practice and can be easily added to existing systems.<br><br>**Bio:**<br>[https://www.tu-braunschweig.de/en/sec/team/rieck](https://www.tu-braunschweig.de/en/sec/team/rieck)<br></details>


### Previous Iteration
This is a reboot of the [Turing Institute](https://www.turing.ac.uk)'s interest group in Privacy and Machine Learning ([old page](https://www.turing.ac.uk/research/interest-groups/privacy-preserving-data-analysis)). We have branched out and expanded to topics at the intersection of Security (not "just" Privacy) and Machine Learning.


<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/Dn_NkH-IEVA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->

### Organizers
- [Prof. Emiliano De Cristofaro](https://emilianodc.com/), UCL  
- [Dr. Giovanni Cherubin](https://giocher.com/), Alan Turing Institute  
- [Prof. Lorenzo Cavallaro](https://s2lab.cs.ucl.ac.uk/people/sullivan), UCL  
- [Dr. Vasilis Mavroudis](https://mavroud.is/), Alan Turing Institute  

